[
  {
    "question": "A startup is building a new API with unpredictable traffic patterns. They want to minimize compute cost without long-term commitments while keeping the option to move workloads between EC2, Fargate, and Lambda over time. Which purchasing model best aligns with the Well-Architected Cost Optimization pillar?",
    "options": {
      "A": "Standard 3-year Reserved Instances for EC2",
      "B": "Zonal Reserved Instances for EC2 and Fargate",
      "C": "Compute Savings Plans with a 1- or 3-year term",
      "D": "EC2 Spot Instances for all production traffic"
    },
    "correctAnswer": "C",
    "explanation": "Compute Savings Plans offer the greatest flexibility across EC2, Fargate, and Lambda while providing significant savings without tying to a specific instance family or region, matching variable usage patterns."
  },
  {
    "question": "A company runs a web application on Amazon EC2 behind an Application Load Balancer in a single Availability Zone. As part of the Reliability pillar, how should the architecture be improved to increase availability? (Select TWO.)",
    "options": {
      "A": "Add another Availability Zone and place EC2 instances in an Auto Scaling group across both AZs.",
      "B": "Replace the ALB with a Gateway Load Balancer.",
      "C": "Use Amazon Route 53 latency-based routing to two records in the same region and AZ.",
      "D": "Enable health checks and cross-zone load balancing on the ALB.",
      "E": "Increase the instance size in the current AZ to handle peak traffic."
    },
    "correctAnswer": "[A, D]",
    "explanation": "Distributing instances across multiple AZs with Auto Scaling and enabling health checks/cross-zone load balancing improves fault tolerance and availability. Simply scaling up in one AZ or changing to GLB does not address AZ failure."
  },
  {
    "question": "A security team requires centralized encryption key management with the ability to audit key usage and enable annual automatic rotation for selected data stores. Which solution aligns with the Security pillar and least privilege best practices?",
    "options": {
      "A": "Use SSE-S3 for all S3 buckets and default EBS encryption",
      "B": "Use AWS KMS customer managed keys (CMKs) with key policies and CloudTrail logging",
      "C": "Use client-side encryption with custom libraries only",
      "D": "Store keys in EC2 instance user data and rotate via cron"
    },
    "correctAnswer": "B",
    "explanation": "KMS customer managed keys provide fine-grained IAM/key policies and CloudTrail audit capabilities. Automatic key rotation is available for symmetric encryption KMS keys and can be enabled to rotate annually. This meets centralized management, auditability, and rotation requirements while maintaining least privilege access control."
  },
  {
    "question": "An operations team wants to improve operational readiness for a business-critical service. According to the Operational Excellence pillar, which actions are MOST appropriate? (Select TWO.)",
    "options": {
      "A": "Adopt Infrastructure as Code (IaC) with automated, tested deployments.",
      "B": "Rely on manual runbooks maintained on a shared drive without version control.",
      "C": "Conduct game days and inject faults to validate incident response.",
      "D": "Disable alarms during deployments to reduce noise.",
      "E": "Use a single admin account with broad permissions to reduce management overhead."
    },
    "correctAnswer": "[A, C]",
    "explanation": "IaC enables controlled, repeatable changes; game days and fault injection validate responses and improve learning. The other options reduce visibility or violate least privilege."
  },
  {
    "question": "A read-heavy microservice frequently fetches the same data from DynamoDB and external APIs, causing high latency. To meet the Performance Efficiency pillar, which approach is BEST?",
    "options": {
      "A": "Increase the DynamoDB table's provisioned write capacity units (WCUs).",
      "B": "Use Amazon ElastiCache to cache API responses and DynamoDB reads.",
      "C": "Switch DynamoDB to on-demand mode only.",
      "D": "Add more application threads to make requests in parallel without caching."
    },
    "correctAnswer": "B",
    "explanation": "Caching with ElastiCache reduces repeated fetch latency and offloads read load, improving performance and efficiency. Other options do not reduce repeated reads or external API latency."
  },
  {
    "question": "A backend processes orders by posting to multiple downstream services. Occasionally, transient errors cause failures and duplicate orders when retries occur. Which design supports the Reliability pillar? (Select TWO.)",
    "options": {
      "A": "Use Amazon SQS to decouple producers and consumers.",
      "B": "Implement idempotency tokens or natural idempotency keys on order operations.",
      "C": "Disable retries and rely on manual reprocessing.",
      "D": "Batch all downstream calls into a single synchronous request.",
      "E": "Use exponential backoff with jitter for retries."
    },
    "correctAnswer": "[B, E]",
    "explanation": "Idempotency ensures safe retries without duplicates, which is critical for preventing duplicate orders. Exponential backoff with jitter is a reliability best practice that prevents retry storms and gives failing systems time to recover. While SQS (option A) provides decoupling benefits, idempotency and proper retry logic (B and E) directly address the stated problem of duplicate orders during retries."
  },
  {
    "question": "An engineering team wants to reduce the environmental impact of a stateless API while controlling cost. What choices best align with the Sustainability pillar? (Select TWO.)",
    "options": {
      "A": "Migrate to AWS Lambda with provisioned concurrency set only for peak hours.",
      "B": "Use Graviton-based instances for compute where possible.",
      "C": "Run large always-on GPU instances for headroom.",
      "D": "Disable Auto Scaling to avoid scaling events.",
      "E": "Deploy to multiple regions with active-active at all times regardless of demand."
    },
    "correctAnswer": "[A, B]",
    "explanation": "Serverless scales to zero off-peak; targeted provisioned concurrency limits waste. Graviton offers better performance-per-watt. The other options increase idle or unnecessary consumption."
  },
  {
    "question": "A company's private subnets host application servers that must read and write data to Amazon S3 without using the public internet and must minimize egress charges through NAT. Which solution follows the Security and Cost Optimization pillars?",
    "options": {
      "A": "Route S3 traffic through a NAT gateway in a public subnet.",
      "B": "Create a Gateway VPC endpoint for S3 and update route tables.",
      "C": "Use an internet gateway and restrict with NACLs only.",
      "D": "Use an S3 access point reachable only from the internet."
    },
    "correctAnswer": "B",
    "explanation": "A Gateway VPC endpoint provides private connectivity to S3 from private subnets and avoids NAT data processing/egress costs, improving security and cost efficiency."
  },
  {
    "question": "A global media site serves large static assets from an S3 bucket to users worldwide. Data transfer out costs are rising, and latency is inconsistent. Which actions best align with the Performance Efficiency and Cost Optimization pillars? (Select TWO.)",
    "options": {
      "A": "Put Amazon CloudFront in front of the S3 origin.",
      "B": "Enable S3 Transfer Acceleration for all downloads to reduce cost.",
      "C": "Use a Gateway VPC endpoint for S3 to reduce internet egress to users.",
      "D": "Compress and cache assets with appropriate Cache-Control headers.",
      "E": "Move the bucket to a more expensive region closer to HQ."
    },
    "correctAnswer": "[A, D]",
    "explanation": "CloudFront caches content at edge locations reducing latency and often lowering data transfer cost. Proper compression and caching reduce origin bytes. Transfer Acceleration can improve speed but generally increases cost."
  },
  {
    "question": "A team wants to reduce deployment risk for a new version of a critical API while maintaining observability and rapid rollback. Which approach aligns with the Operational Excellence pillar?",
    "options": {
      "A": "Deploy in-place to all instances simultaneously to minimize version skew.",
      "B": "Use AWS CodeDeploy blue/green (or canary) with automatic rollback on alarm breaches.",
      "C": "Disable health checks during deployment to avoid false alarms.",
      "D": "Manually copy binaries to servers and restart services during off-hours."
    },
    "correctAnswer": "B",
    "explanation": "Blue/green or canary deployments with automated health checks and rollback reduce blast radius and enable safe, observable changes with quick recovery."
  }
]